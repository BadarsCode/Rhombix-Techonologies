{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Titanic Classification \u2014 End-to-End Guide\n", "\n", "Welcome! This notebook walks you through building a complete machine learning pipeline that predicts passenger survival on the Titanic.\n", "\n", "**You will:**\n", "1. Understand the data and target (Survived).\n", "2. Explore and visualize key factors (gender, age, class, fare, family, embarkation).\n", "3. Engineer features (Title, FamilySize, IsAlone, TicketGroup).\n", "4. Build robust preprocessing with `ColumnTransformer` and `Pipeline`.\n", "5. Train and compare multiple models (Logistic Regression, RandomForest, GradientBoosting; optional XGBoost).\n", "6. Evaluate with cross-validation, confusion matrix, ROC AUC.\n", "7. Interpret feature importance & permutation importance.\n", "8. Export a trained model and generate a Kaggle submission.\n", "\n", "---\n", "### How to use this notebook\n", "- If you're on **Google Colab**:\n", "  - Upload `train.csv` and `test.csv` from Kaggle's Titanic competition to the Colab session (or mount Google Drive and set the paths below).\n", "  - Run the cells from top to bottom.\n", "- If you're on **local Jupyter**:\n", "  - Place `train.csv` and `test.csv` in the same folder as this notebook or update the paths below.\n", "\n", "**Dataset source**: Kaggle Titanic \u2014 Machine Learning from Disaster.\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Setup\n", "Install and import the libraries you need. Skip installs if already available."]}, {"cell_type": "code", "metadata": {"id": "setup"}, "source": ["# If in Colab, uncomment these:\n", "# !pip install scikit-learn pandas numpy matplotlib joblib shap xgboost --quiet\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n", "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n", "                             f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay)\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n", "from sklearn.inspection import permutation_importance\n", "import joblib\n", "import re\n", "import warnings\n", "warnings.filterwarnings('ignore')\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Load data\n", "Set the paths to your CSVs. If you're in Colab, use the file upload panel (left sidebar) and keep these default names."]}, {"cell_type": "code", "metadata": {}, "source": ["TRAIN_PATH = 'train.csv'\n", "TEST_PATH  = 'test.csv'  # used later for submission\n", "\n", "train = pd.read_csv(TRAIN_PATH)\n", "test  = pd.read_csv(TEST_PATH)\n", "train.head()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Quick data check"]}, {"cell_type": "code", "metadata": {}, "source": ["display(train.shape)\n", "display(train.isna().sum())\n", "train.describe(include='all')"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Basic EDA (Exploratory Data Analysis)"]}, {"cell_type": "code", "metadata": {}, "source": ["# Target balance\n", "surv_counts = train['Survived'].value_counts().sort_index()\n", "surv_counts.plot(kind='bar')\n", "plt.title('Target balance: Survived (0 = No, 1 = Yes)')\n", "plt.xlabel('Survived')\n", "plt.ylabel('Count')\n", "plt.show()\n", "\n", "# Survival by Sex\n", "train.groupby('Sex')['Survived'].mean().plot(kind='bar')\n", "plt.title('Survival Rate by Sex')\n", "plt.ylabel('Survival Rate')\n", "plt.show()\n", "\n", "# Survival by Pclass\n", "train.groupby('Pclass')['Survived'].mean().plot(kind='bar')\n", "plt.title('Survival Rate by Passenger Class')\n", "plt.ylabel('Survival Rate')\n", "plt.show()\n", "\n", "# Age distribution by Survival\n", "train[['Age','Survived']].dropna().hist(by='Survived', column='Age', bins=30, sharex=True)\n", "plt.suptitle('Age Distribution by Survival')\n", "plt.show()\n", "\n", "# Embarked vs Survival\n", "train.groupby('Embarked')['Survived'].mean().plot(kind='bar')\n", "plt.title('Survival Rate by Embarked')\n", "plt.ylabel('Survival Rate')\n", "plt.show()\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Feature Engineering\n", "We'll create useful features that capture social status and family structure:\n", "- **Title** extracted from `Name` (e.g., Mr, Mrs, Miss, Master, etc.)\n", "- **FamilySize** = `SibSp + Parch + 1`\n", "- **IsAlone** = 1 if `FamilySize == 1` else 0\n", "- **TicketGroupSize** = number of passengers sharing the same ticket (proxy for group travel)\n", "\n", "We also handle rare titles by grouping them."]}, {"cell_type": "code", "metadata": {}, "source": ["def extract_title(name: str) -> str:\n", "    m = re.search(r',\\s*([^\\.]+)\\.', name)\n", "    return m.group(1).strip() if m else 'Unknown'\n", "\n", "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n", "    df = df.copy()\n", "    df['Title'] = df['Name'].apply(extract_title)\n", "    # Map rare titles\n", "    common = {'Mr','Mrs','Miss','Master'}\n", "    df['Title'] = df['Title'].apply(lambda t: t if t in common else 'Rare')\n", "    \n", "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n", "    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n", "    \n", "    # Ticket group size\n", "    ticket_counts = df['Ticket'].value_counts()\n", "    df['TicketGroupSize'] = df['Ticket'].map(ticket_counts)\n", "    return df\n", "\n", "train_fe = add_engineered_features(train)\n", "test_fe  = add_engineered_features(test)\n", "train_fe[['Name','Title','FamilySize','IsAlone','Ticket','TicketGroupSize']].head()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Preprocessing & Train/Validation Split\n", "We'll impute missing values, one-hot encode categorical variables, and scale numeric features where useful."]}, {"cell_type": "code", "metadata": {}, "source": ["TARGET = 'Survived'\n", "feature_cols = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','IsAlone','Title','TicketGroupSize']\n", "\n", "X = train_fe[feature_cols]\n", "y = train_fe[TARGET]\n", "\n", "X_train, X_valid, y_train, y_valid = train_test_split(\n", "    X, y, test_size=0.2, random_state=42, stratify=y\n", ")\n", "\n", "numeric_features = ['Age','Fare','FamilySize','TicketGroupSize']\n", "categorical_features = ['Pclass','Sex','Embarked','IsAlone','Title']\n", "\n", "numeric_transformer = Pipeline(steps=[\n", "    ('imputer', SimpleImputer(strategy='median')),\n", "    ('scaler', StandardScaler())\n", "])\n", "\n", "categorical_transformer = Pipeline(steps=[\n", "    ('imputer', SimpleImputer(strategy='most_frequent')),\n", "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n", "])\n", "\n", "preprocessor = ColumnTransformer(\n", "    transformers=[\n", "        ('num', numeric_transformer, numeric_features),\n", "        ('cat', categorical_transformer, categorical_features)\n", "    ]\n", ")\n", "\n", "X_train.shape, X_valid.shape"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Baseline Model \u2014 Logistic Regression"]}, {"cell_type": "code", "metadata": {}, "source": ["log_reg = Pipeline(steps=[\n", "    ('prep', preprocessor),\n", "    ('clf', LogisticRegression(max_iter=200, n_jobs=None))\n", "])\n", "\n", "log_reg.fit(X_train, y_train)\n", "preds = log_reg.predict(X_valid)\n", "probs = log_reg.predict_proba(X_valid)[:,1]\n", "\n", "def eval_metrics(y_true, y_pred, y_prob):\n", "    print('Accuracy :', round(accuracy_score(y_true, y_pred), 4))\n", "    print('Precision:', round(precision_score(y_true, y_pred), 4))\n", "    print('Recall   :', round(recall_score(y_true, y_pred), 4))\n", "    print('F1       :', round(f1_score(y_true, y_pred), 4))\n", "    print('ROC AUC  :', round(roc_auc_score(y_true, y_prob), 4))\n", "    \n", "eval_metrics(y_valid, preds, probs)\n", "\n", "cm = confusion_matrix(y_valid, preds)\n", "fig, ax = plt.subplots()\n", "ax.imshow(cm)\n", "ax.set_title('Confusion Matrix \u2014 Logistic Regression')\n", "ax.set_xlabel('Predicted')\n", "ax.set_ylabel('True')\n", "for (i, j), val in np.ndenumerate(cm):\n", "    ax.text(j, i, int(val), ha='center', va='center')\n", "plt.show()\n", "\n", "RocCurveDisplay.from_estimator(log_reg, X_valid, y_valid)\n", "plt.title('ROC Curve \u2014 Logistic Regression')\n", "plt.show()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Model Comparison\n", "We'll try a couple of tree-based models and compare via cross-validation."]}, {"cell_type": "code", "metadata": {}, "source": ["models = {\n", "    'LogisticRegression': LogisticRegression(max_iter=200),\n", "    'RandomForest'     : RandomForestClassifier(n_estimators=400, random_state=42),\n", "    'GradientBoosting' : GradientBoostingClassifier(random_state=42),\n", "    # 'XGBoost'       : XGBClassifier(n_estimators=400, max_depth=4, learning_rate=0.1, subsample=0.9, colsample_bytree=0.9, random_state=42)\n", "}\n", "\n", "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n", "cv_results = {}\n", "for name, clf in models.items():\n", "    pipe = Pipeline(steps=[('prep', preprocessor), ('clf', clf)])\n", "    scores = cross_val_score(pipe, X, y, cv=cv, scoring='accuracy')\n", "    cv_results[name] = (scores.mean(), scores.std())\n", "\n", "for name, (mean_acc, std_acc) in cv_results.items():\n", "    print(f\"{name:18s}  Acc: {mean_acc:.4f} \u00b1 {std_acc:.4f}\")"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Fit Best Model & Interpretability\n", "Pick the best-performing model above (update `best_name` if needed) and inspect feature importance (tree model) and permutation importance."]}, {"cell_type": "code", "metadata": {}, "source": ["best_name = max(cv_results, key=lambda k: cv_results[k][0])\n", "best_clf = models[best_name]\n", "best_pipe = Pipeline(steps=[('prep', preprocessor), ('clf', best_clf)])\n", "best_pipe.fit(X_train, y_train)\n", "print('Best model selected:', best_name)\n", "\n", "# Scores on validation\n", "val_preds = best_pipe.predict(X_valid)\n", "val_probs = best_pipe.predict_proba(X_valid)[:,1] if hasattr(best_pipe.named_steps['clf'], 'predict_proba') else None\n", "eval_metrics(y_valid, val_preds, val_probs if val_probs is not None else val_preds)\n", "\n", "# Feature importance for tree-based models\n", "if hasattr(best_pipe.named_steps['clf'], 'feature_importances_'):\n", "    # Retrieve feature names after preprocessing\n", "    ohe = best_pipe.named_steps['prep'].named_transformers_['cat'].named_steps['onehot']\n", "    num_feats = ['Age','Fare','FamilySize','TicketGroupSize']\n", "    cat_feats = list(ohe.get_feature_names_out(['Pclass','Sex','Embarked','IsAlone','Title']))\n", "    all_feats = num_feats + cat_feats\n", "    importances = best_pipe.named_steps['clf'].feature_importances_\n", "    imp = pd.Series(importances, index=all_feats).sort_values(ascending=False)\n", "    ax = imp.head(20).plot(kind='bar')\n", "    ax.set_title(f'Feature Importance \u2014 {best_name}')\n", "    ax.set_ylabel('Importance')\n", "    plt.show()\n", "\n", "# Permutation importance (model-agnostic)\n", "r = permutation_importance(best_pipe, X_valid, y_valid, n_repeats=10, random_state=42)\n", "perm_imp = pd.Series(r.importances_mean, index=feature_cols).sort_values(ascending=False)\n", "ax = perm_imp.plot(kind='bar')\n", "ax.set_title('Permutation Importance (Validation Set)')\n", "ax.set_ylabel('Importance')\n", "plt.show()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) Train on Full Data & Export Model"]}, {"cell_type": "code", "metadata": {}, "source": ["# Refit on ALL training data using the best model\n", "final_pipe = Pipeline(steps=[('prep', preprocessor), ('clf', models[best_name])])\n", "final_pipe.fit(X, y)\n", "joblib.dump(final_pipe, 'titanic_model.joblib')\n", "print('Saved model to titanic_model.joblib')"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10) Generate Submission for Kaggle"]}, {"cell_type": "code", "metadata": {}, "source": ["# Prepare test features with the exact same engineering as training\n", "X_test = test_fe[feature_cols]\n", "test_pred = final_pipe.predict(X_test)\n", "submission = pd.DataFrame({\n", "    'PassengerId': test['PassengerId'],\n", "    'Survived': test_pred\n", "})\n", "submission.to_csv('submission.csv', index=False)\n", "submission.head()"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11) Use the Model for New Passengers\n", "Example: predict survival for a hypothetical passenger dictionary (same feature keys)."]}, {"cell_type": "code", "metadata": {}, "source": ["def predict_one(passenger_features: dict):\n", "    df = pd.DataFrame([passenger_features])\n", "    # Minimal fields expected: Pclass, Sex, Age, Fare, Embarked, FamilySize, IsAlone, Title, TicketGroupSize\n", "    model = joblib.load('titanic_model.joblib')\n", "    prob = None\n", "    if hasattr(model.named_steps['clf'], 'predict_proba'):\n", "        prob = model.predict_proba(df)[:,1][0]\n", "    pred = model.predict(df)[0]\n", "    return pred, prob\n", "\n", "example = {\n", "    'Pclass': 1,\n", "    'Sex': 'female',\n", "    'Age': 28,\n", "    'Fare': 80,\n", "    'Embarked': 'S',\n", "    'FamilySize': 1,\n", "    'IsAlone': 1,\n", "    'Title': 'Miss',\n", "    'TicketGroupSize': 1\n", "}\n", "\n", "print('Prediction (1=Survived,0=Not):', predict_one(example))"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 12) Next Steps / Ideas\n", "- Hyperparameter tuning with `GridSearchCV` or `RandomizedSearchCV`.\n", "- Use kNN imputation for `Age`.\n", "- Try more features: cabin deck (first letter of Cabin), ticket prefix, age/fare bins.\n", "- Try `XGBoost`/`LightGBM`/`CatBoost` and calibrate probabilities.\n", "- Use `SHAP` for more detailed model explanations.\n", "- Log experiments and metrics with MLflow.\n", "- Build a simple FastAPI/Streamlit app around your trained model."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}